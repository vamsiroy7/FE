import pandas as pd
import numpy as np
from scipy.fft import fft
import pywt
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima.model import ARIMA
from fastdtw import fastdtw
from scipy.spatial.distance import euclidean
from scipy.stats import entropy
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from statsmodels.tsa.stattools import grangercausalitytests

# Define parameters for synthetic data generation
num_accounts = 100  # Number of unique accounts
num_months = 24  # Number of months of data
start_date = '2022-01-01'  # Start date for the time series

# Generate account IDs
account_ids = [f'ACC{i:03d}' for i in range(1, num_accounts + 1)]

# Generate a date range for the time series
date_range = pd.date_range(start=start_date, periods=num_months, freq='M')

# Create an empty DataFrame to hold the time series data
data = pd.DataFrame()

# Populate the DataFrame with synthetic data
for account_id in account_ids:
    credit_limit = np.random.uniform(5000, 20000)  # Random credit limit for each account
    interest_rate = np.random.uniform(10, 25)  # Random interest rate
    account_age = np.random.randint(1, 60)  # Random account age in months
    account_data = pd.DataFrame({
        'AccountID': account_id,
        'Date': date_range,
        'TransactionAmount': np.random.uniform(100, 5000, size=num_months),  # Random transaction amounts
        'CreditLimit': credit_limit,
        'CurrentBalance': np.random.uniform(0, credit_limit, size=num_months),  # Random current balance
        'PaymentAmount': np.random.uniform(0, 3000, size=num_months),  # Random payment amounts
        'LateFees': np.random.uniform(0, 100, size=num_months),  # Random late fees
        'CreditUtilizationRatio': lambda df: df['CurrentBalance'] / df['CreditLimit'],  # Credit utilization ratio
        'NumTransactions': np.random.randint(1, 50, size=num_months),  # Random number of transactions
        'AnnualFee': np.random.uniform(0, 100),  # Random annual fee
        'InterestRate': interest_rate,
        'AccountAge': account_age,
        'MinimumPaymentDue': np.random.uniform(20, 200, size=num_months),  # Random minimum payment due
        'AccountStatus': np.random.choice(['Active', 'Inactive'], size=num_months, p=[0.95, 0.05])  # Random account status
    })
    account_data['CreditUtilizationRatio'] = account_data['CurrentBalance'] / credit_limit
    data = pd.concat([data, account_data], ignore_index=True)

# Add temporal aggregations
data['Year'] = data['Date'].dt.year
data['Month'] = data['Date'].dt.month
data['WeekOfYear'] = data['Date'].dt.isocalendar().week
data['DayOfWeek'] = data['Date'].dt.dayofweek
data['DayOfMonth'] = data['Date'].dt.day

# Add Fourier Transform features
def add_fourier_features(df, column, n_components=10):
    result = fft(df[column].values)
    df[f'{column}_fft_real'] = result.real
    df[f'{column}_fft_imag'] = result.imag
    for i in range(1, n_components + 1):
        df[f'{column}_fft_real_{i}'] = result[i].real
        df[f'{column}_fft_imag_{i}'] = result[i].imag
    return df

data = data.groupby('AccountID').apply(lambda x: add_fourier_features(x, 'TransactionAmount'))

# Add Wavelet Transform features
def add_wavelet_features(df, column, wavelet='db1', level=3):
    coeffs = pywt.wavedec(df[column], wavelet, level=level)
    for i, coeff in enumerate(coeffs):
        df[f'{column}_wavelet_coeff_{i}'] = coeff
    return df

data = data.groupby('AccountID').apply(lambda x: add_wavelet_features(x, 'TransactionAmount'))

# Add STL decomposition features
def add_stl_features(df, column):
    stl = STL(df[column], seasonal=13)
    result = stl.fit()
    df[f'{column}_trend'] = result.trend
    df[f'{column}_seasonal'] = result.seasonal
    df[f'{column}_resid'] = result.resid
    return df

data = data.groupby('AccountID').apply(lambda x: add_stl_features(x, 'TransactionAmount'))

# Add ARIMA features
def add_arima_features(df, column, order=(1, 1, 1)):
    model = ARIMA(df[column], order=order)
    model_fit = model.fit()
    df[f'{column}_arima_pred'] = model_fit.predict()
    df[f'{column}_arima_resid'] = model_fit.resid
    return df

data = data.groupby('AccountID').apply(lambda x: add_arima_features(x, 'TransactionAmount'))

# Add DTW distance features
def add_dtw_distance(df, reference_series):
    df['DTW_Distance'] = df.apply(lambda x: fastdtw(x['TransactionAmount'], reference_series, dist=euclidean)[0], axis=1)
    return df

# Assume the first account's TransactionAmount is the reference series
reference_series = data[data['AccountID'] == 'ACC001']['TransactionAmount'].values
data = data.groupby('AccountID').apply(lambda x: add_dtw_distance(x, reference_series))

# Add entropy features
def add_entropy_features(df, column):
    df[f'{column}_entropy'] = entropy(df[column].value_counts())
    return df

data = data.groupby('AccountID').apply(lambda x: add_entropy_features(x, 'TransactionAmount'))

# Add correlation with external data features
# Example: Assume 'external_data' is a DataFrame with economic indicators
external_data = pd.DataFrame({
    'Date': date_range,
    'EconomicIndicator': np.random.uniform(0, 100, size=num_months)
})

data = data.merge(external_data, on='Date', how='left')
data['Corr_TransactionAmount_EconomicIndicator'] = data.groupby('AccountID')['TransactionAmount'].rolling(window=3).corr(data['EconomicIndicator']).reset_index(level=0, drop=True)

# Add PCA features
def add_pca_features(df, columns, n_components=2):
    pca = PCA(n_components=n_components)
    pca_result = pca.fit_transform(df[columns])
    for i in range(n_components):
        df[f'pca_{i+1}'] = pca_result[:, i]
    return df

data = add_pca_features(data, ['TransactionAmount', 'CurrentBalance', 'PaymentAmount'])

# Add autoencoder features for anomaly detection
def add_autoencoder_features(df, columns):
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[columns])

    input_dim = scaled_data.shape[1]
    encoding_dim = 3

    input_layer = Input(shape=(input_dim,))
    encoder = Dense(encoding_dim, activation="relu")(input_layer)
    decoder = Dense(input_dim, activation="sigmoid")(encoder)

    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer="adam", loss="mean_squared_error")

    autoencoder.fit(scaled_data, scaled_data, epochs=50, batch_size=16, shuffle=True, verbose=0)

    encoded_data = autoencoder.predict(scaled_data)
    mse = np.mean(np.power(scaled_data - encoded_data, 2), axis=1)
    df['Autoencoder_MSE'] = mse
    return df

data = add_autoencoder_features(data, ['TransactionAmount', 'CurrentBalance', 'PaymentAmount'])

# Add Granger causality test features
def add_granger_causality_features(df, target_col, feature_col, maxlag=3):
    test_result = grangercausalitytests(df[[target_col, feature_col]], maxlag=maxlag, verbose=False)
    p_values = [round(test[0]['ssr_ftest'][1], 4) for test in test_result.values()]
    df[f'GrangerCausality_{feature_col}_pvalues'] = p_values
    return df

data = data.groupby('AccountID').apply(lambda x: add_granger_causality_features(x, 'TransactionAmount', 'CurrentBalance'))

# Display the first few rows of the DataFrame
print(data.head())

# Save the DataFrame to a CSV file for review
data.to_csv('combined_advanced_features_banking_time_series_data.csv', index=False)
