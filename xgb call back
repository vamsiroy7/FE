import xgboost as xgb

params = {...}
n_estimators = 100
folds = ...  # Your KFold or other CV strategy

# Storage for metrics
evals_result = []

class MetricsCallback(xgb.callback.TrainingCallback):
    def after_iteration(self, model, epoch, evals_log):
        if not evals_log:
            return False
        evals_result.append(evals_log)
        return False  # continue training

# Cross-validation
cv_results = xgb.cv(
    params=params,
    dtrain=xgb.DMatrix(data, label=labels),
    num_boost_round=n_estimators,
    folds=folds,
    callbacks=[MetricsCallback()]
)

# Now 'evals_result' contains metrics from each iteration
